ä¸å®šæœŸæŒç»­æ›´æ–°ä¸­ï¼Œä¸Šæ¬¡æ›´æ–°æ—¥æœŸï¼š2025.02.23 â†“â†“â†“

## â†“ 2025
    å¤§è§„æ¨¡RL + æ€ç»´é“¾å¸¦æ¥çš„æ”¶ç›Š
#### 22 Jan 2025, DeepSeek, DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning -> [Link](https://arxiv.org/abs/2501.12948)
#### 22 Jan 2025, DeepSeek, Kimi k1.5: Scaling Reinforcement Learning with LLMs -> [Link](https://arxiv.org/abs/2501.12599)

## â†“ 2024
    ðŸ’¡ PPOå˜ä½“å‡ºçŽ°ï¼Œä¸€ä¸ªæ–¹å‘æ˜¯åŽ»æŽ‰critic modelï¼Œä»£è¡¨ä½œGRPOã€RLOOç­‰ï¼Œå¦ä¸€æ–¹å‘æ˜¯rule baseçš„RFTï¼Œä»£è¡¨ä½œopenaiçš„Rule Based Rewards for Language Model Safetyã€å­—èŠ‚çš„REFTç­‰
#### 
#### 27 Dec 2024, DeepSeek, DeepSeek-V3 Technical Report -> [Link](https://arxiv.org/abs/2412.19437)
#### 20 Dec 2024, openai, Deliberative Alignment: Reasoning Enables Safer Language Models -> [Link](https://arxiv.org/pdf/2412.10560)
> * openaiåœ¨å®‰å…¨é¢†åŸŸçš„æŽ¢ç´¢ï¼Œé€šè¿‡æ€ç»´é“¾å¯¹å®‰å…¨å‡†åˆ™è¿›è¡Œå›žå¿†ï¼Œä»Žè€ŒæŒ‡å¯¼æ¨¡åž‹çš„å›žç­”
> ![alt text](img/DeliberativeAlignment.png)
> * å…·ä½“åšæ³•ï¼š
>   * sftï¼š
>       * æ•°æ®ç”Ÿæˆï¼šprompt + å®‰å…¨åˆ†ç±»cat --å®‰å…¨å‡†åˆ™spec + base model--> cot + output
>       * æ•°æ®è¿‡æ»¤ï¼šprompt + å®‰å…¨åˆ†ç±»cat + cot + output --å®‰å…¨å‡†åˆ™spec + generative reward model--> score
>       * è®­ç»ƒï¼šprompt + cot + output
>   * RLï¼š
>       * é‡‡æ ·ï¼šprompt + å®‰å…¨åˆ†ç±»cat --sft model--> cot + output
>       *  RMï¼šprompt + å®‰å…¨åˆ†ç±»cat + output å®‰å…¨å‡†åˆ™spec + generative reward model--> score
>       * ä¼˜åŒ–policy
>   * ä½¿ç”¨æ€ç»´é“¾å¯¹æ¨¡åž‹å›žç­”è¿›è¡Œè¯„ä¼°ï¼Œä»Žè€ŒæŒ‡å¯¼æ¨¡åž‹çš„å›žç­”
#### 8 Dec 2024, Zhipu AI, Does RLHF Scale? Exploring the Impacts from Data, Model, and Method -> [Link](https://arxiv.org/pdf/2412.06000)
> * æ™ºè°±çš„å¼ºåŒ–ç»„çš„å·¥ä½œï¼ŒæŽ¢ç´¢RL Scale Law
> * ä»Žé‡‡æ ·æ¬¡æ•°ã€reward modelå¤§å°ã€ppo vs grpo vs BONã€åˆ°PRM vs ORMç­‰
#### 17 Jun 2024, NVIDIA, Nemotron-4 340B Technical Report -> [Link](https://arxiv.org/pdf/2406.11704)
> * æ•°æ®ï¼š
>   * ç”Ÿæˆå•è½®promptï¼šç”Ÿæˆå®è§‚ä¸»é¢˜ -> å­ä¸»é¢˜ -> ç”Ÿæˆé—®é¢˜ -> é—®é¢˜ç»†åŒ–
>   * å¯¹è¯ç”Ÿæˆ
>   * åˆæˆåå¥½æ•°æ®ç”Ÿæˆï¼šå¥–åŠ±æ¨¡åž‹å‡†ç¡®çŽ‡ > å¤§åž‹è¯­è¨€æ¨¡åž‹ä½œä¸ºè¯„åˆ¤çš„å‡†ç¡®çŽ‡
> * è®­æ³•ï¼š
>   * åˆ†æ®µçš„ç›‘ç£å¼å¾®è°ƒï¼šä»£ç SFT -> é€šç”¨ç›‘ç£å¼å¾®è°ƒï¼ˆGeneral SFTï¼‰+ 2%çš„ä»£ç 
>   * åå¥½å¾®è°ƒ:
>     * DPOæŸå¤±+SFTæŸå¤± ï¼ˆ160Kä¸ªæ ·ä¾‹çš„åå¥½æ•°æ®é›†ï¼Œè¦†ç›–äº†å¤šæ ·çš„ä»»åŠ¡ç±»åž‹ï¼‰
>     * 3è½® å¥–åŠ±é©±åŠ¨çš„åå¥½ä¼˜åŒ–RPOï¼ˆ30ä¸‡ä¸ªæ ·æœ¬çš„åå¥½æ•°æ®é›†ï¼‰
>     * Reward Modelï¼š
>       * ç»†ç²’åº¦å¤šå¤´çš„Regression Modelï¼šHelpfulness, Correctness, Coherence, Complexity, Verbosity
>       * ç”¨çš„æ—¶å€™å¤šç»´åº¦åŠ æƒå¹³å‡
#### 24 July 2024, OpenAI,Rule Based Rewards for Language Model Safety -> [Link](https://cdn.openai.com/rule-based-rewards-for-language-model-safety.pdf)
> * OpenAI åœ¨å¤§æ¨¡åž‹å­¦ä¹ å®‰å…¨æ€§å¼ºåŒ–ä¸Šï¼Œåˆå›žå½’äº†ç»å…¸çš„åŸºäºŽè§„åˆ™çš„å¥–åŠ±(RBR)æœºåˆ¶ã€‚ï¼ˆä¹‹å‰å°è¯•äº†PRMçš„æ–¹å¼ä½œæ•°å­¦ï¼‰
#### 2 Jun 2024, University of Chicago, RLOO: BoNBoN Alignment for Large Language Models and the Sweetness of Best-of-n Sampling -> [Link](https://arxiv.org/pdf/2406.00832v1) 
> * å¤šä¸ªæ¶ˆèžå®žéªŒï¼Œè¯æ˜Žå¯ä»¥å°†ppoé€€åŒ–ä¸ºreinforceç®—æ³•ï¼Œå³åŽ»æŽ‰critic model
#### 23 May 2024, University of Virginia, SimPO: Simple Preference Optimization with a Reference-Free Reward -> [Link](https://arxiv.org/pdf/2405.14734)
>   * ä»Žå®žéªŒæ¥çœ‹ï¼Œæ¯”è¾ƒæ‰Žå®žï¼Œå¯¹æ¯”äº†å¾ˆå¤šç®—æ³•ï¼Œä¼˜äºŽDPOï¼ŒKTOï¼ŒIPOï¼ŒORPOï¼ŒRDPO
>   * ä¸ä¼šå¢žåŠ å›žå¤é•¿åº¦ï¼Œä¸€äº›å·¥ä½œè®¤ä¸ºé•¿åº¦æ˜¯ä¸€ç§åå¥½çš„hack
>   * SimPOä½¿ç”¨æ”¹ç”¨æ ·æœ¬é•¿åº¦åšç¼©æ”¾æ¥ä»£æ›¿å‚è€ƒæ¨¡åž‹ï¼ŒåŽé¢å†åŠ äº†ä¸€ä¸ªç±»ä¼¼äºŽmarginçš„é¡¹ï¼ˆå®šå€¼ï¼‰
>   * psï¼šçœ‹åˆ°æœ‰ä¸å°‘å…„å¼Ÿç”¨èµ·æ¥è¿˜æ˜¯å®¹æ˜“é£ž--Link
>   * å›¾ï¼šsimpo vs dpo
> ![alt text](img/SimPO.png)
#### 14 May 2024, DeepMind, Understanding the performance gap between online and offline alignment algorithms -> [Link](https://arxiv.org/pdf/2405.08448)
>   * ä»Žä»¥ä¸‹å‡ ä¸ªæ–¹é¢è§£é‡Šå’Œè¯æ˜Žäº†åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ›´å¥½
>     * æ•°æ®è´¨é‡ï¼šç¦»çº¿ç®—æ³•çš„æ•°æ®å¯èƒ½æ¥è‡ªæ¬¡ä¼˜ç­–ç•¥ï¼Œå¯¼è‡´å…¶è®­ç»ƒæ•ˆæžœè¾ƒå·®ã€‚å¦‚æžœç¦»çº¿æ•°æ®æ¥è‡ªé«˜è´¨é‡çš„ç­–ç•¥ï¼Œç¦»çº¿ç®—æ³•çš„æ€§èƒ½ä¼šæ›´å¥½ã€‚
>     * æ¨¡åž‹è§„æ¨¡çš„å½±å“ï¼šæ¨¡åž‹è§„æ¨¡çš„å˜å¤§å¯ä»¥ç¼©å°ç¦»çº¿å’Œåœ¨çº¿çš„å·®è·
>     * è¿‡ä¼˜åŒ–çŽ°è±¡ï¼šåœ¨çº¿å’Œç¦»çº¿ç®—æ³•åœ¨éƒ½å‡ºçŽ°è¿‡ä¼˜åŒ–çŽ°è±¡ï¼Œä½†æ˜¯åœ¨ç›¸åŒæ—¶é—´èŠ‚ç‚¹ï¼Œåœ¨çº¿ç®—æ³•æ›´ä¼˜
>     * ...ç­‰
#### 13 May 2024, UIUC, RLHF Workflow: From Reward Modeling to Online RLHF -> [Link](https://arxiv.org/pdf/2405.07863)
> * åŸºäºŽ DPO çš„åœ¨çº¿è¿­ä»£ RLHF
>   * ä¸»è¦æµç¨‹ï¼š1) é¦–å…ˆä½¿ç”¨å¼€æºåå¥½æ•°æ®é›†ä¸Šè¿è¡Œ DPOï¼› 2ï¼‰æœ€å¤§åŒ–å·®å¼‚é‡‡æ ·ï¼ˆæŽ¢ç´¢ï¼‰responseï¼Œç”¨reward modelæ‰“åˆ†ï¼Œä½¿ç”¨DPOè¿›è¡Œè¿­ä»£ã€‚
> * ã€Œå‰ç½®å·¥ä½œã€18 Dec 2023, UIUC, Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint (ICML2024) -> [Link](https://arxiv.org/pdf/2312.10560)
#### 7 May 2024, DeepSeek-AI, DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model -> [Link](https://arxiv.org/abs/2405.04434)
> * ã€Œå‰ç½®å·¥ä½œã€ä½¿ç”¨Group Relative Policy Optimization (GRPO) çš„ç®—æ³•æ¥è¿›è¡Œå¯¹é½è®­ç»ƒï¼Œç®—æ³•å‡ºè‡ªDeepSeekMath
>   * é¿å…ä½¿ç”¨è¯„ä»·æ¨¡åž‹ï¼šä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•é€šå¸¸éœ€è¦ä¸€ä¸ªä¸Žç­–ç•¥æ¨¡åž‹åŒç­‰å¤§å°çš„è¯„ä»·æ¨¡åž‹æ¥æä¾›è®­ç»ƒä¿¡å·ã€‚GRPOç®—æ³•ä¸ä½¿ç”¨è¯„ä»·æ¨¡åž‹ï¼Œè€Œæ˜¯ä»Žä¸€ç»„è¾“å‡ºçš„å¾—åˆ†ä¸­ä¼°è®¡åŸºçº¿ã€‚
>   * ç»„ç›¸å¯¹ä¼˜åŒ–ï¼šå¯¹äºŽæ¯ä¸ªè¾“å…¥ï¼ŒGRPOä¼šä»Žæ—§ç­–ç•¥ä¸­é‡‡æ ·ä¸€ç»„è¾“å‡ºï¼Œç„¶åŽä¼˜åŒ–ç­–ç•¥æ¨¡åž‹ä»¥æœ€å¤§åŒ–ç‰¹å®šçš„ç›®æ ‡å‡½æ•°ã€‚è¿™ä¸ªè¿‡ç¨‹åˆ©ç”¨ç»„å†…è¾“å‡ºçš„ç›¸å¯¹å¾—åˆ†æ¥è°ƒæ•´ç­–ç•¥ï¼Œæé«˜å…¶é€‰æ‹©åˆé€‚å›žåº”çš„æ¦‚çŽ‡ã€‚
>   * DeepSeek-V2çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šç¬¬ä¸€é˜¶æ®µæ˜¯åŸºäºŽé€»è¾‘å’Œæ•°å­¦æŽ¨ç†ä»»åŠ¡çš„å¯¹é½ï¼Œç¬¬äºŒé˜¶æ®µæ˜¯åŸºäºŽäººç±»åå¥½çš„å¯¹é½ã€‚
#### 1 May 2024, University of Illinois Urbana-Champaign, Iterative Preference Learning from Human Feedback- BridgingTheory and Practice for RLHF under KL-constraint -> [Link](https://arxiv.org/pdf/2312.11456)
> * æ–‡ç« ä¸»è¦åŒ…æ‹¬ä¸¤ä¸ªéƒ¨åˆ†:
>   * æ•´ä¸ª RLHF çš„ç†è®ºï¼Œ
>   * åœ¨ RLHF ä¸­åŠ å…¥ online data çš„å¥½å¤„
> * [å›´è§‚] ä½œè€…ä¹‹ä¸€ï¼š[Wei Xiongï¼šä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥åšonline RLHF/DPO?](https://zhuanlan.zhihu.com/p/688806682)
#### 1 May 2024, University of California(CMU), SPPO: Self-Play Preference Optimization for Language Model Alignment -> [Link](https://arxiv.org/abs/2405.00675)
> * SPPOé€šè¿‡è‡ªå¯¹å¼ˆçš„æ–¹å¼è¿›è¡Œæ¨¡åž‹å¾®è°ƒï¼Œæ¯ä¸€è½®ä¸­ï¼Œæ¨¡åž‹åœ¨ç”±æ¨¡åž‹æœ¬èº«ç”Ÿæˆçš„åˆæˆæ•°æ®ä¸Šè¿›è¡Œå¾®è°ƒï¼Œå¹¶ç”±åå¥½æ¨¡åž‹æ¥åˆ¤æ–­åˆæˆæ•°æ®çš„èƒœè´ŸçŽ‡ã€‚å¯ä»¥çœ‹æˆæ˜¯åœ¨æ¨¡æ‹Ÿä¸¤ä¸ªçŽ©å®¶ä¹‹é—´çš„å¯¹å¼ˆï¼Œå…¶ä¸­æ¯ä¸ªçŽ©å®¶éƒ½è¯•å›¾ä¼˜åŒ–è‡ªå·±çš„æ¨¡åž‹ä»¥æœ€å¤§åŒ–å…¶èƒœçŽ‡ã€‚
#### April 29, 2024, bigcode, StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation -> [Link](https://mp.weixin.qq.com/s/g2VvvKCy577XdYQKEmAXzw)
> * ä»£ç ç”Ÿæˆä»»åŠ¡çš„è‡ªæˆ‘å¯¹é½
#### 23 Apr 2024, Arizona State University, Insights into Alignment: Evaluating DPO and its Variants Across Multiple Tasks -> [Link](https://arxiv.org/pdf/2404.14723v1?_immersive_translate_auto_translate=1)
> * å®žéªŒå¯¹DPOã€IPOã€KTOã€CPOå››ç§ç®—æ³•ä¸Šåœ¨13 ä¸ªæµ‹è¯•é›†ä¸Šè¿›è¡Œå¯¹æ¯”å®žéªŒï¼ŒåŸºçº¿æ˜¯Mistral+SFTï¼Œç»“æžœä¸ºbase+KTOæ•ˆæžœæœ€å¥½ï¼ˆä¸éœ€è¦SFTï¼‰
#### 29 Apr 2024, Peking University, DPO Meets PPO: Reinforced Token Optimization for RLHF -> [Link](https://arxiv.org/pdf/2404.18922)
> * å¯¹æ¯”äº†å‡ ç§levelçš„å¼ºåŒ–ç®—æ³•ï¼Œå¹¶æå‡ºRTOï¼Œä¸­æ–‡å‚è€ƒ -> [Link](https://zhuanlan.zhihu.com/p/698536446)
> * token-level rewardï¼Œå¸¸è§çš„åœºæ™¯æ˜¯detoxification
> * å»ºæ¨¡ä¸ºstep(sub-sentence)-level rewardçš„PRM
> * ä»Žsentence levelçš„reference dataä¸­é—´æŽ¥å­¦åˆ°çš„token-level rewardï¼ŒRTOå±žäºŽæ­¤ç±»æ–¹æ³•
#### 18 Apr 2024, University of Chinese Academy of Sciences&Microsoft Research AI4Science, Token-level Direct Preference Optimizationc -> [Link](https://arxiv.org/html/2404.11999v1?_immersive_translate_auto_translate=1)
> * TDPOçš„ä¸€ä¸ªå…³é”®ç‰¹ç‚¹æ˜¯ï¼šç›´æŽ¥åœ¨tokençº§åˆ«ä¸Šåº”ç”¨åå¥½ä¼˜åŒ–ï¼Œå¯ä»¥æ›´ç²¾ç»†åœ°æŽ§åˆ¶æ¨¡åž‹è¾“å‡º
> * ![alt text](img/Token-levelDirectPreferenceOptimizationc.png)
#### 16 Apr 2024, Tsinghua University, Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study -> [Link](https://arxiv.org/html/2404.10719v2?_immersive_translate_auto_translate=1)
> * DPOçš„æ ¹æœ¬æ€§é—®é¢˜ï¼šåœ¨DPOå…¬å¼çš„ä¸­ï¼Œå½“x=0ï¼Œlog(Ïƒ(x)) = 0.693ï¼Œå³å½“æ¨¡åž‹ä¸å¤Ÿè‡ªä¿¡çš„æ—¶å€™ï¼Œä¼šå¯¹ä¸€äº›åå¥½æ¨¡ç³Šçš„æ ·æœ¬è¿›è¡Œæ­£å‘æ›´æ–°ï¼Œå¯èƒ½ä¼šå‘å±•å‡ºä¸€äº›å¥‡æ€ªçš„è¡Œä¸ºï¼Œè¿™äº›è¡Œä¸ºåœ¨è®­ç»ƒæ•°æ®ä¸­çœ‹ä¸åˆ°ï¼Œä½†å´å—åˆ°äº†DPOçš„é’çã€‚
> * PPOé€šå¸¸é€šè¿‡ä¸€ç³»åˆ—æŠ€æœ¯å¦‚ç­–ç•¥æ¢¯åº¦ã€ä¼˜åŠ¿å‡½æ•°ä»¥åŠç­–ç•¥çš„è¿›æ­¥è£å‰ªï¼ˆclippingï¼‰ï¼Œæ¥é¿å…è¿‡å¤§çš„ç­–ç•¥æ›´æ–°ï¼Œä»Žè€Œåœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶æä¾›æ›´å¹³æ»‘ã€æ›´ç¨³å®šçš„å­¦ä¹ è·¯å¾„
#### Apr 2024, fuyao, Llama 3 Opens the Second Chapter of the Game of Scale -> [Link](https://yaofu.notion.site/Apr-2024-Llama-3-Opens-the-Second-Chapter-of-the-Game-of-Scale-efff1c0c185f4008af673b78faf83b61)
> * fuyaoå¤§ä½¬å¯¹llama3çš„åˆ†æžï¼Œåœ¨æ•°æ®é‡è§é¡¶ä»¥åŽï¼Œå¼ºåŒ–å°†ä¼šå‘æŒ¥ä¸»è¦ä½œç”¨
#### 18 Apr 2024, Stanford, From r to Qâˆ—: Your Language Model is Secretly a Q-Function -> [Link](https://arxiv.org/pdf/2404.12358.pdf)
> * DPO ä½œè€…ï¼Œç»“åˆå¼ºåŒ–çš„åŽŸç†å¯¹DPOè¿›è¡Œåˆ†æžå’Œè§£é‡Š
> * ä¸­æ–‡ç¡¬æ ¸åˆ†æžï¼š[é™ˆé™ˆï¼šDPOæ–°ä½œYour Language Model is Secretly a Q-Functionè§£è¯»ï¼Œä¸ŽOPENAI Q* çš„è”ç³»ï¼Ÿ](https://zhuanlan.zhihu.com/p/693746297)
#### 15 Apr 2024, microsoft, WizardLM-2 (è¹²ä¸€ä¸‹è¿™ç¯‡è®ºæ–‡)
> * ã€Œå‰ç½®å·¥ä½œã€18 Aug 2023, microsoft, WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct (WizardLM-2ä¸­çš„RLEIF)
> * ã€Œå‰ç½®å·¥ä½œã€31 May 2023, openai, Let's Verify Step by Step (WizardLM-2ä¸­çš„PRM)
> * ã€Œå‰ç½®å·¥ä½œã€28 Mar 2024, sDPO: Don't Use Your Data All at Once (stage dpoçš„ä¸€ç§)
> * WizardLM-2å¯¹é½è®­ç»ƒè¿‡ç¨‹å€¼å¾—å‚è€ƒ
> * æ¨¡åž‹ä¸‹è½½å’Œæ¨¡åž‹ç®€å•ä»‹ç»ï¼š[Replete-AI/WizardLM-2-7b Â· Hugging Face](https://huggingface.co/posts/WizardLM/329547800484476)
> ![alt text](/img/wizardlm2.png)
#### 3 Apr 2024, ZhiPu, ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline -> [Link](https://mp.weixin.qq.com/s/lg7ueR9b-om0ecUEoT4x8w)
> * ç”¨Math-Critique modelç»™å¤šä¸ªç­”æ¡ˆæ‰“åˆ†ï¼Œè¿›è¡ŒRejective Fine-tuningï¼Œå†åˆ°DPO
> * dpoå˜ç§ï¼Œé€šè¿‡åŠ å…¥é¢å¤–çš„lossä½œä¸ºæ­£åˆ™é¡¹ï¼Œ
#### 1 Apr 2024, Zhipu AI&Tsinghua University, ChatGLM-RLHF: Practices of Aligning Large Language Models with Human Feedback -> [Link](https://arxiv.org/pdf/2404.00934)
> * æ•°æ®å·¥ä½œï¼šæ€Žä¹ˆä¿è¯åå¥½çš„ä¸€è‡´æ€§
> * åˆ†æžäº†ç›¸å¯¹RLHFåœ¨æŸäº›ç»´åº¦ä¸ºä»€ä¹ˆä¸å¦‚SFTï¼šå¥–åŠ±æ¨¡åž‹çš„å‡†ç¡®æ€§ï¼ŒRLHFçš„è®­ç»ƒæ•°æ®ä¸ŽSFTè®­ç»ƒæ•°æ®çš„åˆ†å¸ƒçš„å·®å¼‚
#### 26 Mar 2024, University of Edinburgh&University of Illinois Urbana-Champaign, Alignment Guidebook -> [Link](https://efficient-unicorn-451.notion.site/Alignment-Guidebook-e5c64df77c0a4b528b7951e87337fa78)
> * æå‡ºäº† ConditionalOnLine RLHF RLHFï¼Œä½¿ç”¨ä¸€ç§æ–°é¢–çš„æ¡ä»¶å¥–åŠ±æ¨¡åž‹ï¼Œå¯ä»¥è°ƒå’Œä¸åŒç±»åž‹çš„äººç±»åå¥½ï¼ˆä¾‹å¦‚ï¼Œå¤šæ­¥éª¤æŽ¨ç†å‡†ç¡®æ€§ã€æœ‰ç”¨æ€§ã€æ— å®³æ€§ï¼‰ï¼Œå¹¶è¿›è¡Œä¸‰è½®åœ¨çº¿ RLHFã€‚
> * åŒæ—¶ï¼Œæåˆ°äº†ä¸€äº›è®­ç»ƒç»†èŠ‚ï¼Œæ¯”å¦‚åœ¨PPOé˜¶æ®µä¸ºä»€ä¹ˆéœ€è¦åœ¨æŠŠactor freeze 50æ­¥? -> Link
#### 5 Mar 2024, PERL: Parameter Efficient Reinforcement Learning from Human Feedback -> [Link](https://arxiv.org/pdf/2403.10704.pdf)
> * ä½¿ç”¨RLHFçš„åå¥½æ•°æ®ï¼Œè¿›è¡Œå‚æ•°é«˜æ•ˆçš„å¯¹é½
#### 12 Mar 2024, ORPO: Monolithic Preference Optimization without Reference Model -> [Link](https://arxiv.org/abs/2403.07691)
> * SFT çš„æŸå¤±å‡½æ•°å¯¹äºŽrejected dataæ²¡æœ‰æƒ©ç½šé¡¹ï¼ŒSFT ä¹‹åŽæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„ç”Ÿæˆæ¦‚çŽ‡æœ‰å¯èƒ½åŒæ—¶ä¸Šå‡
#### 4 Mar 2024, Anthropic, The Claude 3 Model Family: Opus, Sonnet, Haiku -> [Link](https://assets.anthropic.com/m/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf)
> * å‚è€ƒæ¨¡åž‹ï¼šhttps://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf
> * referenceè·Ÿå¼ºåŒ–ç›¸å…³çš„è®ºæ–‡ï¼š
>   * [10] Anthropic, â€œChallenges in evaluating AI systems.â€ October, 2023. https://www.anthropic.com/index/evaluating-ai-systems.
>   * [11] Anthropic, â€œRed Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned.â€ August, 2022. https://www.anthropic.com/index/ red-teaming-language-models-to-reduce-harms-methods-scaling-behaviors-and-lessons-learned.
>   * [12] Anthropic, â€œThe Capacity for Moral Self-Correction in Large Language Models.â€ February, 2023. https://www.anthropic.com/index/the-capacity-for-moral-self-correction-in-large-language-models.
>   * [14] Anthropic, â€œFrontier Threats Red Teaming for AI Safety.â€ July, 2023. https://www.anthropic.com/index/frontier-threats-red-teaming-for-ai-safety.
#### 2 Mar 2024, ByteDance, Improving Reinforcement Learning from Human Feedback Using Contrastive Rewards -> [Link](https://arxiv.org/abs/2403.07708)
> * ä½¿ç”¨å¯¹æ¯”Rewardæ”¹è¿›RLHF
#### 29 Feb 2024, DeepMind, ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL -> [Link](https://arxiv.org/html/2402.19446v1?_immersive_translate_auto_translate=1)
> * è§£å†³LLMsåœ¨å¤šä¸ªå›žåˆä¸­æ™ºèƒ½åœ°æœç´¢å’Œæ•´åˆä¿¡æ¯çš„èƒ½åŠ›
#### 5 Feb 2024, DeepSeek-AI, DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models -> [Link](https://arxiv.org/html/2402.03300v3?_immersive_translate_auto_translate=1)
> * é¦–æ¬¡æå‡ºç¾¤ç›¸å¯¹ç­–ç•¥ä¼˜åŒ– ï¼ˆGRPOï¼‰ï¼Œè¿™æ˜¯è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– ï¼ˆPPOï¼‰ çš„å˜ä½“ï¼Œå¢žå¼ºæ•°å­¦æŽ¨ç†èƒ½åŠ›ï¼ŒåŒæ—¶ä¼˜åŒ– PPO çš„å†…å­˜ä½¿ç”¨ã€‚
#### 18 Jan 2024, Meta, Self-Rewarding Language Models -> [Link](https://arxiv.org/abs/2401.10020)
> * å®žçŽ°ç®€å•ï¼Œå¯ä»¥çœ‹æˆrjçš„åŠ å¼ºç‰ˆã€‚ç»“åˆwizardçš„æŒ‡ä»¤è¿›åŒ–ã€LLamaçš„reject samplingã€DPOè¿›è¡Œå¾ªçŽ¯è¿­ä»£å¯¹é½
> * ç±»ä¼¼æ–¹æ³•ï¼š2 May 2024 D2PO: Discriminator-Guided DPO with Response Evaluation Models
#### 17 Jan 2024, ByteDance, ReFT: Reasoning with Reinforced Fine-Tuning -> [Link](https://arxiv.org/abs/2401.08967)
> * rule baseçš„RFTï¼Œè·Ÿopenaiçš„RFTå¾®è°ƒç±»ä¼¼
> * è§£å†³çš„é—®é¢˜ï¼šCoTè®­ç»ƒæ ·æœ¬é€šå¸¸åªåŒ…å«ä¸€æ¡æ­£ç¡®çš„æŽ¨ç†è·¯å¾„ï¼Œä½†æ˜¯åœ¨æ•°å­¦é¢†åŸŸä¸‹ï¼Œä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆå¾€å¾€æœ‰å¤šä¸ªè§£æ³•ï¼ˆå³ä¸åŒçš„æŽ¨ç†è·¯å¾„ï¼‰
> * æ–¹æ³•ï¼š
>   * å…ˆç”¨cotæ•°æ®å¾®è°ƒ1-2ä¸ªepoch
>   * å†ç”¨rule reward modeè¿›è¡Œå¼ºåŒ–å­¦ä¹ ï¼Œé€šè¿‡æ¨¡åž‹é‡‡ç”¨èŽ·å¾—å¤šç§æŽ¨ç†è·¯å¾„ï¼Œä»Žè€Œè¾¾åˆ°æ³›åŒ–çš„æ•ˆæžœ
> ![alt text](img/ReFT.png)
#### 11 Jan 2024, Fudan NLP Lab & Fudan Vision and Learning Lab, Secrets of RLHF in Large Language Models Part II: Reward Modeling -> [Link](https://arxiv.org/abs/2401.06080)
> * å¤æ—¦è‡ªç„¶è¯­è¨€å®žéªŒå®¤ï¼Œé‚±é”¡é¹è€å¸ˆç»„çš„å·¥ä½œ
> * ã€Œå‰ç½®å·¥ä½œã€11 Jul 2023, Fudan NLP Lab & Fudan Vision and Learning Lab, Secrets of RLHF in Large Language Models Part I: PPO
#### 16 Jan 2024, ICLR2024, CPPO: Continual Learning for Reinforcement Learning with Human Feedback -> [Link](https://openreview.net/pdf?id=86zAUE80pP)
#### 8 Jan 2024, Google&CMU, A Minimaximalist Approach to Reinforcement Learning from Human Feedback (SPO) -> [Link](https://arxiv.org/pdf/2401.04056.pdf)
#### 2 Jan 2024,  University of California, SPIN: Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models -> [Link](https://arxiv.org/pdf/2401.01335)

##  â†“ 2023
    ðŸ’¡ ä»¥å„ç§DPOçš„å˜ä½“ä¸ºä¸»é¢˜

#### 14 Dec 2023, OpenAI, Weak-to-strong generalization -> [Link](https://openai.com/research?topics=safety-alignment)
> * [OpenAIçš„Weak-to-Strongåœ¨è¯´ä»€ä¹ˆ](https://zhuanlan.zhihu.com/p/673427353)
#### 1 Dec 2023, Tsinghua University, RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback -> [Link](https://arxiv.org/abs/2312.00849)
> * è§£å†³å¤šæ¨¡æ€æ¨¡åž‹çš„å¹»è§‰é—®é¢˜
#### 14 Nov 2023, Tencent , Adversarial Preference Optimization -> [Link](https://arxiv.org/abs/2311.08045)
#### 28 Mar 2022, ICLR2022, The N Implementation Details of RLHF with PPO -> [Link](https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo)
* https://openreview.net/forum?id=Hl6jCqIp2j
#### 16 Oct 2023 , The Chinese University of Hong Kong, ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models -> [Link](https://arxiv.org/pdf/2310.10505.pdf)
> * åŽ»æŽ‰äº†A2Cé‡Œçš„critic modelï¼Œåªä½¿ç”¨rewardæ›´æ–°æ¨¡åž‹ï¼Œä½†æ˜¯ä¼šéžå¸¸ä¸ç¨³å®šï¼Œæ‰€ä»¥é€šè¿‡å‡æŽ‰ä¸€ä¸ªbaselineæ¥é™ä½Žrewardçš„æ–¹å·®
#### 6 Oct 2023, UCL&DeepMind&CMU, Confronting Reward Model Overoptimization with Constrained RLHF-> [Link](https://arxiv.org/abs/2310.04373)
> * ICLR 2024 Spotlightï¼Œä¸»è¦è§£å†³ç”±äºŽå¥–åŠ±æ¨¡åž‹åªæ˜¯äººç±»è¯„ä¼°çš„ä¸å®Œç¾Žä»£ç†ï¼Œå› æ­¤å­˜åœ¨è¿‡ä¼˜åŒ–çš„é£Žé™© 
> * å±žäºŽè¿›ä¸€æ­¥è§£å†³openaiçš„Scaling Laws for Reward Model Overoptimizationä¸­çš„é—®é¢˜
#### 4 Oct 2023, University of Cambridge, Reward Model Ensembles Help Mitigate Overoptimization -> [Link](https://arxiv.org/abs/2310.02743)
> * ICLR 2024 Spotlightï¼ŒåŒæ ·è§£å†³rlhfçš„è¿‡åº¦ä¼˜åŒ–é—®é¢˜
#### 28 Sep 2023, Johns Hopkins University(éœæ™®é‡‘æ–¯å¤§å­¦)&Tencent AI Lab, The Trickle-down Impact of Reward (In-)consistency on RLHF -> [Link](https://arxiv.org/pdf/2309.16155.pdf)
> * ICLR 2024, è§£å†³åå¥½æ•°æ®ä¸€è‡´æ€§é—®é¢˜
#### 4 Oct 2023, University of Cambridge, Reward Model Ensembles Help Mitigate Overoptimization -> [Link](https://arxiv.org/abs/2310.02743)
> *ICLR 2024, åŒæ ·è§£å†³rlhfçš„è¿‡åº¦ä¼˜åŒ–é—®é¢˜
#### 25 Sep 2023, UC Berkeley&CMU, LLAva-RLHF: Aligning Large Multimodal Models with Factually Augmented RLHF -> [Link](https://llava-rlhf.github.io/)
> * å¤šæ¨¡æ€çš„RLHF
#### 25 Sep 2023, CMU, Aligning Large Multimodal Models with Factually Augmented RLHF -> [Link](https://arxiv.org/abs/2309.14525)
#### 27 Jul 2023, OpenAI, Superalignment with Jan Leike
> * https://mp.weixin.qq.com/s/P45Mi_dxFxRGjk57XhFJSQ
> * https://podcasts.google.com/feed/aHR0cHM6Ly9heHJwb2RjYXN0LmxpYnN5bi5jb20vcnNz/episode/MThlYjczZGItZmYxZS00MDU0LWJmOGYtZGRhNWM0ODkzNGM0
#### 19 Jul 2023, Meta, Llama 2: Open Foundation and Fine-Tuned Chat Models -> [Link](https://arxiv.org/abs/2307.09288)
> * ç»“åˆäº†Anthropicè®ºæ–‡ä¸­çš„æ€æƒ³ä½¿ç”¨äº†ä¸¤ç§RM
#### 29 May 2023, Stanford, Direct Preference Optimization(DPO): Your Language Model is Secretly a Reward Model
> * DPOçš„å„ç§æ”¹è¿›ç‰ˆï¼š
>   * ã€Œæ”¹è¿›è®­ç»ƒæµç¨‹ã€28 Mar 2024, sDPO: Don't Use Your Data All at Once -> [Link](https://arxiv.org/abs/2403.19270)
>   * ã€Œæ”¹è¿›lossã€2 Feb 2024, Stanford, KTO: Model Alignment as Prospect Theoretic Optimization -> [Link](https://arxiv.org/abs/2402.01306)
>   * ã€Œæ”¹è¿›lossã€18 Oct 2023, Deepmind, A General Theoretical Paradigm to Understand Learning from Human Preferences (IPO)
>   * ã€Œæ”¹è¿›lossã€ 25 November 2023, A note on DPO with noisy preferences & relationship to IPO -> [Link](https://ericmitchell.ai/cdpo.pdf) 
>       * é€šè¿‡åŠ å…¥label_smoothingæ”¹è¿›dpoå¯¹åå¥½å™ªå£°çš„å½±å“
>   * ã€Œæ”¹è¿›æµç¨‹ã€15 Apr 2024, (Trust Region)TR-DPO: Learn Your Reference Model for Real Good Alignment 
>       * é€šè¿‡æ›´æ–°ref model å¯ä»¥å¸¦æ¥æ›´å¥½çš„æ•ˆæžœï¼Œæ³¨æ„ï¼šéœ€è¦è®¾ç½®åˆé€‚çš„ref modelæ›´æ–°è¶…å‚

## â†“ 2022
    ðŸ’¡ openaiã€Anthropicæ—©æœŸå¯¹ppoçš„æŽ¢ç´¢

#### 19 Oct 2022, OpenAI, Scaling Laws for Reward Model Overoptimization (RLHFçš„scaling lawï¼šPPOé˜¶æ®µåº”è¯¥ä½•æ—¶åœæ­¢è®­ç»ƒ)
> * https://arxiv.org/abs/2210.10760
#### 12 Apr 2022, Anthropic, Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback
> * ä¸­æ–‡ç‰ˆï¼šhttps://zhuanlan.zhihu.com/p/605133974
> * æœ‰å¸®åŠ©ä¸Žæ— å®³çš„è®­ç»ƒæ•°æ®æ¯”ä¾‹
#### 25 Mar 2022, ICLR Blog Track, The 37 Implementation Details of Proximal Policy Optimization ï¼ˆRLè®­ç»ƒçš„37ä¸ªTrickï¼‰
> * https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/
#### 4 Mar 2022, OpenAI, Training language models to follow instructions with human feedbackï¼ˆOpenAIåœ¨LLMå¼ºåŒ–åšæ³•ï¼‰
> * https://arxiv.org/abs/2203.02155

## â†“ 2016
    ðŸ’¡ ppoçš„åŸºç¡€

#### 2016, ICLR, High-dimensional continuous control using generalized advantage estimation. (GAEï¼Œå‡ ä¹Žæ‰€æœ‰æœ€å…ˆè¿›çš„policy gradientç®—æ³•å®žçŽ°é‡Œé¢éƒ½ä½¿ç”¨äº†è¯¥æŠ€æœ¯ã€‚)
> * https://arxiv.org/pdf/1506.02438.pdf
> * https://zhuanlan.zhihu.com/p/45107835?from=groupmessage&utm_id=0
> * https://zhuanlan.zhihu.com/p/402198744

